# uvicorn main:app --reload
from fastapi import FastAPI, HTTPException
import json
import requests

app = FastAPI()

# 0. json ë°ì´í„° ë¡œë“œ
def load_health_data():
    with open("data/health_data.json", encoding="utf-8") as f:
        return json.load(f)


# 1. Mock API êµ¬í˜„
# ê±´ê°•ê²€ì§„ ë°ì´í„°ë¥¼ ì œê³µí•˜ëŠ” ê°„ë‹¨í•œ JSON íŒŒì¼ ë˜ëŠ” Mock Serverë¥¼ ìƒì„±í•˜ì„¸ìš”.
# **API ì—”ë“œí¬ì¸íŠ¸**: `GET /api/health/{patientId}`

@app.get("/api/health/{patientId}")
def get_health_data(patientId: str):
    data = load_health_data()
    if patientId in data:
        return {
            "status": "success",
            "data": data[patientId]
        }
    else:
        raise HTTPException(status_code=404, detail="Patient not found")


# 2. Ollama LLM ì—°ë™
MODEL_NAME = "llama3.1:8b" # ollama pull llama3.1:8b
OLLAMA_API_URL = "http://localhost:11434/api/generate"

def ask_ollama(prompt: str):
    response = requests.post(
        OLLAMA_API_URL,
        json={
            "model": MODEL_NAME,
            "system": "ì§ˆë¬¸ì„ ë°˜ë³µí•˜ì§€ ë§ê³ , ë°”ë¡œ 3ë¬¸ì¥ìœ¼ë¡œ ê°„ëµí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.",
            "prompt": prompt
        },
        stream=True
    )

    if response.status_code != 200:
        raise HTTPException(status_code=500, detail="LLM request failed")

    full_text = ""
    for line in response.iter_lines():
        if not line:
            continue
        try:
            json_line = json.loads(line.decode("utf-8"))
            if "response" in json_line:
                full_text += json_line["response"]
        except json.JSONDecodeError:
            continue

    return full_text.strip() or "LLM ì‘ë‹µì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."


# 3. ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ
# ì‚¬ìš©ì ì§ˆë¬¸ â†’ API ë°ì´í„° ì¡°íšŒ â†’ LLM í”„ë¡¬í”„íŠ¸ ìƒì„± â†’ ì‘ë‹µ
@app.post("/qna")
def ask_health_question(patientId: str, question: str):
    data = load_health_data()

    if patientId not in data:
        raise HTTPException(status_code=404, detail="Patient not found")

    health_info = json.dumps(data[patientId], ensure_ascii=False, indent=3)

    # ğŸ”¹ LLMì—ê²Œ ë³´ë‚¼ í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = f"""
            ë‹¹ì‹ ì€ ì˜ë£Œ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            ë‹¤ìŒì€ í™˜ìì˜ ê±´ê°•ê²€ì§„ ë°ì´í„°ì…ë‹ˆë‹¤:

            {health_info}
            
            ë°ì´í„° êµ¬ì¡°:
            - overviewList: ì‚¬ìš©ìì˜ ê±´ê°•ê²€ì§„ ë°ì´í„°
            - referenceList: ê° ë°ì´í„° ë‹¨ìœ„ ë° ì •ìƒ ë²”ìœ„/ì§ˆí™˜ ì˜ì‹¬ ì—¬ë¶€

            ì‚¬ìš©ìì˜ ì§ˆë¬¸: "{question}"

            ì‘ë‹µ ê·œì¹™:
            1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ê³ , ìˆ˜ì¹˜ëŠ” ë°ì´í„° ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
            2. ê²€ì§„ ë‚ ì§œë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”.
            3. ì§ˆë¬¸í•˜ëŠ” ê²ƒì—ë§Œ ë‹µë³€í•˜ì„¸ìš”.
            4. ìˆ˜ì¹˜ì˜ ì •ìƒ ë²”ìœ„/ì£¼ì˜/ì§ˆí™˜ì˜ì‹¬ ì—¬ë¶€ë¥¼ êµ¬ë¶„í•´ì„œ ì•Œë ¤ì£¼ì„¸ìš”.
            5. ê°„ë‹¨í•œ ì¡°ì–¸ì„ ì¶”ê°€í•˜ì„¸ìš”.
            
            
            ì˜ˆì‹œ:
            - ì§ˆë¬¸ 1: "ìµœê·¼ ê±´ê°•ê²€ì§„ ê²°ê³¼ëŠ” ì–´ë•Œìš”?"
            - ë‹µë³€ 1: "8ì›” 15ì¼ ê±´ê°•ê²€ì§„ ê²°ê³¼ì…ë‹ˆë‹¤. BMI 23.5ë¡œ ì •ìƒ, í˜ˆë‹¹ 95mg/dLë¡œ ì •ìƒ ë²”ìœ„ì— ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ í˜ˆì••ì´ 125/82ë¡œ ì•½ê°„ ë†’ì€ í¸ì´ë‹ˆ ì—¼ë¶„ ì„­ì·¨ë¥¼ ì¤„ì´ì‹œê¸¸ ê¶Œí•©ë‹ˆë‹¤."
            
            - ì§ˆë¬¸ 2: "ì½œë ˆìŠ¤í…Œë¡¤ ìˆ˜ì¹˜ê°€ ì–´ë•Œìš”?"
            - ë‹µë³€ 2: "ì´ ì½œë ˆìŠ¤í…Œë¡¤ 190mg/dLë¡œ ì •ìƒ ë²”ìœ„(200 ë¯¸ë§Œ)ì…ë‹ˆë‹¤. HDL 60mg/dLë„ ìš°ìˆ˜í•œ ìˆ˜ì¤€ì´ë„¤ìš”. í˜„ì¬ ìƒíƒœë¥¼ ìœ ì§€í•˜ì‹œë©´ ë©ë‹ˆë‹¤."
            """

    answer = ask_ollama(prompt)

    return answer